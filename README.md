# Nelder-Mead-Downhill-Simplex

## Introduction
The Nelder‚ÄìMead simplex method [49, 50] is an algorithm for finding the local minima of a function $f:R^n‚ÜíR, called the objective function in optimization parlance. The idea
behind it is very geometric. It starts with a suitably chosen initial simplex64 in Rn and
evolves it according to certain rules. The simplex undergoes iterative expansions, contrac-
tions, and displacements, and if everything goes according to plan, it moves ‚Äúdownhill‚Äù
toward a local minimum of f . The term ‚Äúdownhill simplex‚Äù was used in [53] to refer to
this behavior. A significant merit of the algorithm is that it is gradient-free, that is, it does
not require a knowledge of the function‚Äôs gradient, which may be difficult or expensive
to compute. Consequently, it may be applied to minimize nondifferentiable functions,
but see the caveats in Section 18.3.
18.2 The algorithm
The algorithm works in iterative cycles. I will explain what is done in one cycle. The
cycles are repeated until certain convergence criteria are met. The calculations involve
four parameters, Œ≤ r , Œ≤e , Œ≤c , and Œ≤ s , which permit some customization of the algorithm,
although almost always these are taken as
Œ≤ r = 1,
Œ≤e = 2,
1
Œ≤c = ,
2
1
Œ≤s = .
2
Now, without further ado, let us begin with the description of the algorithm.
Rank the vertices. Let us write x(i ) ‚àà Rn , i = 0, 1, . . . , n, for the simplex‚Äôs n + 1 vertices
at the start of a cycle, and let yi = f (x(i ) ) be the function values at those vertices.
Three of the n + 1 vertices play key roles in what happens next. The best vertex is
64 A simplex in n dimensions is the generalization of a triangle (two dimensions) and a tetrahedron (three
dimensions). It is the convex hull of n + 1 points in the n-dimensional Euclidean space. Equivalently, it
may be thought of as the ‚Äúsolid‚Äù object formed by a set of n vectors emanating from a common point in the
n-dimensional space. If the vectors form a linearly independent set, then the simplex has a positive n-dimensional
volume; otherwise it is a degenerate simplex.
193194
Chapter 18. The Nelder‚ÄìMead downhill simplex
one with the lowest function value (it‚Äôs best because we are aiming to minimize the
function). The worst vertex is one with the highest function value. The next to worst
vertex is one with the highest function value after excluding the worst vertex. We
write ia, i y, and i z for the indices of the best, next to worst, and worst vertices,
respectively, because a, y, and z are the first, next to last, and last letters of the
alphabet.65
The definitions of ia, i y, and i z are not quite unambiguous. What are the best,
next to worst, and worst vertices if the vector of the function values is [7, 7, 7, 7] or
[5, 5, 1, 1, 7, 7]? We partially remove the ambiguity by requiring that ia, i y, and i z
be distinct. Stated formally, we require that
ia = i y,
yi a = min yi ,
i
i y = i z,
i z = ia,
yi z = max yi ,
yi y = max yi
if n ‚â• 2.
(18.1a)
i =i z
i
Thus, if the function values are [7, 7, 7, 7], then ia = 0, i z = 1, i y = 2 will do,
but ia = i z = i y = 0 is not acceptable. Similarly, if the function values are
[5, 5, 1, 1, 7, 7], then ia will have to be one of 2 or 3; and either of the choices i z = 4,
i y = 5 or i z = 5, i y = 4 will do.66
In defining the best, next to worst, and worst vertices above I have assumed implic-
itly that the simplex has at least three vertices, that is, the dimension of space n ‚â• 2.
The one-dimensional case, n = 1, is special; the simplex is a line segment, and hence
it has only two vertices. From the definition of i y it follows that i y = ia. Therefore
picking distinct ia, i y, and i z is impossible. We make an exception in the case of
n = 1 and change the first line of the conditions (18.1a) to
ia = i y = i z
if n = 1.
(18.1b)
The ambiguities noted above are not the only ones. There are quite a few decisions
in the rest of the algorithm that call for tie-breakings of various sorts, e.g., making
comparisons with ‚Äú<‚Äù versus ‚Äú‚â§‚Äù. I will gloss over the precise tie-breaking rules‚Äîas
did Nelder and Mead in their paper‚Äîtherefore your program may not work exactly
as someone else‚Äôs that breaks the ties differently. See [40] for a set of precise tie-
breaking rules that lead to a uniquely defined Nelder‚ÄìMead algorithm.
Reflect. Compute the centroid xÃÇ of the simplex face opposite to x(i z) ,
xÃÇ =
1
n i =i z
x(i ) ,
(18.2)
compute the ‚Äúreflection‚Äù x(r ) of x(i z) through xÃÇ,
x(r ) ‚àí xÃÇ = ‚àíŒ≤ r (x(i z) ‚àí xÃÇ),
(18.3a)
and let y (r ) = f (x(r ) ). If Œ≤ r = 1, as it almost always is, then x(r ) is the true reflection
of x(i z) through xÃÇ because then x(r ) ‚àí xÃÇ = xÃÇ ‚àí x(i z) . The reflection diagram in
Figure 18.1 illustrates this.
At this point the algorithm branches into four cases, depending on the relationship
between the value of y (r ) and the vertex values yi , i = 0, 1, . . . , n.
65 Here I am breaking away from the tradition in mathematics of using single letters for variable names because
I could not think of suitable replacements for the double-letter names i a, i y, and i z. From the programming
point of view, however, these are quite natural; programmers routinely use multiletter symbols for variables.
66
Here, and everywhere else in this book for that matter, array indices begin with zero.18.2. The algorithm
195
x(i z)
x(i z)
xÃÇ
xÃÇ
reflection
x(r )
x(i z)
expansion
x(r )
x(i z)
x(e)
x(c)
xÃÇ
x(c)
xÃÇ
inner contraction
outer contraction
x(i a)
shrink
x(r )
Figure 18.1: These diagrams show all possible transformations of a Nelder‚ÄìMead sim-
plex in two dimensions (n = 2) with the standard choices of the parameter
values: Œ≤ r = 1, Œ≤e = 2, Œ≤c = 1/2, Œ≤ s = 1/2.
Case 1: y (r ) < yi a
This is a terrific occurrence; what used to be the worst vertex changes to the
best vertex upon reflection. Thus, the line that connects x(i z) through xÃÇ to
x(r ) appears to be a direction of steep descent of the function f . Take advan-
tage of the opportunity, and expand the simplex in that direction by a factor
of Œ≤e . Thus, calculate a point x(e) such that
x(e) ‚àí xÃÇ = Œ≤e (x(r ) ‚àí xÃÇ),
(18.3b)
and let y (e) = f (x(e) ). Usually one takes Œ≤e = 2. This is illustrated in the
expansion diagram in Figure 18.1. Replace the vertex x(i z) with the better of
x(e) and x(r ) , and end the iteration cycle.
Remark 18.1. This is slightly different from Nelder and Mead‚Äôs original
prescription which is the following: Pick x(e) if y (e) < yi a ; else pick x(r ) .
Case 2: yi a ‚â§ y (r ) < yi y
Replace the vertex x(i z) with x(r ) , and end the iteration cycle.
Case 3: yi y ‚â§ y (r ) < yi z
Replacing x(i z) by x(r ) under these conditions will constitute some improve-
ment, but it will not be very useful since the worst vertex remains the worst
vertex after the replacement. Thus, try another tack: push x(r ) toward the196
Chapter 18. The Nelder‚ÄìMead downhill simplex
centroid xÃÇ by a factor of Œ≤c to arrive at a point x(c) ,
x(c) ‚àí xÃÇ = Œ≤c (x(r ) ‚àí xÃÇ),
(18.3c)
and let y (c) = f (x(c) ). Usually one takes Œ≤c = 1/2. The outer contraction
diagram in Figure 18.1 illustrates this. If y (c) < y (r ) , accept x(c) as a replace-
ment for the vertex x(i z) and end the cycle; otherwise shrink the simplex (see
below) and end the cycle.
Case 4: yi z ‚â§ y (r )
In this case a reflection is of no use at all since the worst point becomes even
worse. Try another tack: push x(i z) toward the centroid xÃÇ by a factor of Œ≤c
to arrive at a point x(c) ,
x(c) ‚àí xÃÇ = Œ≤c (x(i z) ‚àí xÃÇ),
(18.3d)
and let y (c) = f (x(c) ). The inner contraction diagram in Figure 18.1 illustrates
this. If y (c) < yi z , accept x(c) as a replacement for the vertex x(i z) and end the
cycle; otherwise shrink the simplex (see below) and end the cycle.
Shrink. The shrink process referred to in cases 3 and 4 above shrinks the simplex toward
its best vertex x(i a) by moving all other vertices toward x(i a) by a factor of Œ≤ s :
)
x(inew
‚àí x(i a) = Œ≤ s (x(i ) ‚àí x(i a) ) for all i = ia.
(18.3e)
Usually one takes Œ≤ s = 1/2. This is illustrated in the shrink diagram in Figure 18.1.
Are we there yet? There is no universally accepted stopping criterion for the Nelder‚Äì
Mead algorithm. Nelder and Mead‚Äôs original article [49] suggested the criterion
'
( 
(1 n
)
(y ‚àí »≥)2 < Œµ,
n i =0 i
motivated by applications in statistics, where »≥ is the average of the yi ‚Äôs, and Œµ is a
prescribed measure of accuracy. The stopping criterion proposed in [53] is
2
|yi z ‚àí yi a |
|yi z | + |yi a |
<Œµ
for some prescribed Œµ. Either criterion is met if the variation of the objective func-
tion‚Äôs values over the simplex‚Äôs vertices is small. The simplex‚Äôs size does not enter
into the consideration.
Wright [80] quotes others who have suggested
max x(i ) ‚àí x(i a) ‚â§ Œµ max(1, x(i a) )
i =i a
for the stopping criterion, where Œµ is a prescribed measure of accuracy. This crite-
rion is met if the simplex is sufficiently small. The objective function‚Äôs values do
not enter into the consideration.
The following hybrid criterion accounts for both the simplex‚Äôs size and the objec-
tive function‚Äôs values. It is quite robust and has worked well for my purposes, so
that‚Äôs what I prescribe for our program.18.3. Problems with the Nelder‚ÄìMead algorithm
197
The user supplies a number, h, that establishes a ‚Äúlength scale‚Äù in Rn . A good h
should be of an order of magnitude commensurate with the features of interest in
the objective function‚Äôs domain. The length scale h serves dual purposes. For one
thing, it enters the stopping criterion described below, and for another thing, it is
used to set the simplex‚Äôs initial size.
The user also supplies a dimensionless number, œÑ (for tolerance), that determines the
algorithm‚Äôs accuracy. The program lets Œµ = h √ó œÑ and declares that the algorithm
has converged if
x(i z) ‚àí x(i a) ‚â§ Œµ and
|yi z ‚àí yi a | ‚â§ Œµ2 .
(18.4)
This assumes that Œµ is suitable for measuring the ‚Äúsmallness‚Äù of both x and y quan-
tities. It also assumes that the objective function behaves like a quadratic at the
minimizing point, which is generally the case if the function is smooth. You are
free to replace these generic conditions with others which you deem better suited
to problems of special interest to you.
To avoid runaway computations, should the algorithm fail to converge, the program
keeps a count of the number of times it evaluates the objective function. It halts the
iterations if the evaluation count exceeds a user-specified ceiling.